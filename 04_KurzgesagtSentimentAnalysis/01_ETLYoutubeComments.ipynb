{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85534bb2-7642-4898-953e-5ec9d11f07ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import html\n",
    "import getpass\n",
    "import warnings\n",
    "from time import time\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from langdetect import detect, DetectorFactory\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb2f4c-4c9e-4ba1-9c46-bbd949e3895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadKeyYotube() -> str:\n",
    "    \n",
    "    # Load the API key of youtube v3 found in .env\n",
    "    \n",
    "    with open(\"../.env\",\"r\") as iJSON:\n",
    "        key = json.load(iJSON)[\"keys\"][\"key_youtube\"]\n",
    "    return key\n",
    "\n",
    "def GetVideoComments(youtube:build, video_id:str) -> \"list(dict)\":\n",
    "\n",
    "    '''\n",
    "    This function retrieves the comment, date, user and likes of a youtube video\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    video_id: str\n",
    "        id of youtube video\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        list(dict): List of diccionaries containing the comment, date, user and likes of the comment\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "\n",
    "    >>> from googleapiclient.discovery import build\n",
    "    >>> my_video = \"dGiQaabX3_o\"\n",
    "    >>> youtube = build('youtube', 'v3', developerKey=api_key) # replace by your api key\n",
    "    >>> comments = get_video_comments(my_video) \n",
    "\n",
    "    '''\n",
    "    \n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    iteration = 1\n",
    "    while True:\n",
    "        \n",
    "        # Show on screen current pagination\n",
    "        if (iteration % 10) == 0:\n",
    "            print(f\"\\tPages checked: {iteration}\", end=\"\\r\")\n",
    "        \n",
    "        # Calling youtube to retrieve comments\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            pageToken=next_page_token,\n",
    "            maxResults=10000\n",
    "        ).execute()\n",
    "\n",
    "        # Formating into a dict the data requested\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comment_date = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "            comment_user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            comment_likes = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "\n",
    "            comment_data = {\n",
    "                'comment': comment,\n",
    "                'published_date': comment_date,\n",
    "                'name': comment_user,\n",
    "                'likes': comment_likes\n",
    "            }\n",
    "            \n",
    "            comments.append(comment_data)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "        # End while lopp if there is no more pages to retrieve comments\n",
    "        if not next_page_token:\n",
    "            break\n",
    "            \n",
    "        iteration += 1\n",
    "    \n",
    "    print(\"\\nAll done\")\n",
    "    return comments\n",
    "\n",
    "def GetLanguage(text:str) -> str:\n",
    "    \n",
    "    # This function detect the language of a text. If there is no enough evidence\n",
    "    # to define the language returns \"undefined\"\n",
    "\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"undefined\"\n",
    "\n",
    "# The following classes represent a Objectâ€“role modeling (ORM) of the expected tables\n",
    "# found in the database with the corresponding constraints. It's useful to validate data\n",
    "# before insert\n",
    "    \n",
    "class Languages(declarative_base()):\n",
    "    __tablename__ = 'languages'\n",
    "\n",
    "    # Here Integer represents SERIAL in id column\n",
    "    id_language = Column(Integer, primary_key=True, nullable=False, unique=True)\n",
    "    code = Column(String(9), nullable=False, unique=True)\n",
    "    \n",
    "class Titles(declarative_base()):\n",
    "    __tablename__ = 'titles'\n",
    "\n",
    "    id_video = Column(String(12), primary_key=True, nullable=False, unique=True)\n",
    "    title = Column(String, nullable=False)\n",
    "    \n",
    "class Users(declarative_base()):\n",
    "    __tablename__ = 'users'\n",
    "    \n",
    "    # Here Integer represents SERIAL in id column\n",
    "    id_user = Column(Integer, primary_key=True, nullable=False, unique=True)\n",
    "    name = Column(String, nullable=False)\n",
    "    \n",
    "class Comments(declarative_base()):\n",
    "    __tablename__ = 'comments'\n",
    "\n",
    "    # Here Integer represents SERIAL in id column\n",
    "    id_comment = Column(Integer, primary_key=True, nullable=False, unique=True)\n",
    "    comment = Column(Text, nullable=True)\n",
    "    published_date = Column(DateTime, nullable=True)\n",
    "    likes = Column(Integer, nullable=True)\n",
    "    id_user = Column(Integer, nullable=False)\n",
    "    id_video = Column(String, nullable=False)\n",
    "    id_language = Column(Integer, nullable=False)\n",
    "\n",
    "class DataValidator:\n",
    "\n",
    "    '''\n",
    "    This class validates data before inserting into a database\n",
    "    '''\n",
    "\n",
    "    def __init__(self, engine:sqlalchemy.create_engine,\n",
    "                orm_model:sqlalchemy.orm.declarative_base) -> None:\n",
    "        \n",
    "        self.engine = engine\n",
    "        self.Session = sessionmaker(bind=self.engine, autoflush=False)\n",
    "        self.orm_model = orm_model\n",
    "    \n",
    "    def ValidateData(self, dataframe:pd.DataFrame, errors:str=\"stop\") -> list:\n",
    "        \n",
    "        '''\n",
    "        Validate if the data is not violating any restriction by defined for the ORM model\n",
    "        in a database\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        dataframe: pd.DataFrame\n",
    "            A pandas DataFrame to verify each row. It's expected\n",
    "            the same name in the columns of dataframe and the database \n",
    "\n",
    "        error: str, [\"stop\",\"ignore\"]; default=\"stop\"\n",
    "            How the errors should be handle if some registers are not valid.\n",
    "            errors=\"ignore\" ignore the current register and continue validating. Current register is not valid\n",
    "            errors=\"stop\" ends the execution of the script. All the table is not valid\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        list: A list containing the names of the indexes of the dataframe that should be dropped\n",
    "\n",
    "        '''\n",
    "\n",
    "        session = self.Session()\n",
    "        bad_rows = []\n",
    "        \n",
    "        for row in dataframe.itertuples():\n",
    "\n",
    "            row = row._asdict()\n",
    "            index = row.pop(\"Index\")\n",
    "            \n",
    "            # If flush failed. It means there is a constraint violation\n",
    "            try:\n",
    "                model = self.orm_model(**row)\n",
    "                session.add(model)\n",
    "                session.flush()\n",
    "                \n",
    "            except SQLAlchemyError as ErrorDataType:\n",
    "                \n",
    "                if errors == \"ignore\":\n",
    "                    bad_rows.append(index)\n",
    "                    warnings.warn(f\"Data validation failed: {ErrorDataType}\")\n",
    "                else:\n",
    "                    session.rollback()\n",
    "                    session.close()\n",
    "                    if errors == \"stop\":\n",
    "                        raise Exception(f\"Data validation failed: {ErrorDataType}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"Argument 'error' not valid: {errors}. Expected ['stop', 'ignore']\")\n",
    "        \n",
    "        # Undo all changes in the database and close session\n",
    "        session.rollback()\n",
    "        print(\"Done\")\n",
    "        session.close()\n",
    "        \n",
    "        return bad_rows\n",
    "\n",
    "def Extract(video_names:list, videos_id:list, api_key:str) -> \"list(dict)\":\n",
    "\n",
    "    '''\n",
    "    This function extract youtube comments based on the video id\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_names: list\n",
    "        List containing the titles of the youtube videos. Useful to inform current state of extraction\n",
    "    \n",
    "    videos_id: list\n",
    "        List containing the ids of the youtube videos. It's used to extract comments\n",
    "        \n",
    "    api_key: str\n",
    "        API key to request info from youtube\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    list(dict): A list of diccionaries with the comments and metadata about them\n",
    "    \n",
    "    '''\n",
    "\n",
    "    start = time()\n",
    "    print(\"Exctracting comments\")\n",
    "\n",
    "    # Set up YouTube Data API client\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Call the function to retrieve comments for the specified video\n",
    "    full_comment_metadata_videos = []\n",
    "    for name, ID in zip(video_names, videos_id):\n",
    "\n",
    "        print(f\"Retrieving from video: {name}\")\n",
    "        video_comments = GetVideoComments(youtube, ID)\n",
    "        full_comment_metadata_videos.append(video_comments)\n",
    "\n",
    "        print(f\"Comments retrieved: {len(video_comments)}\")\n",
    "\n",
    "    print(f\"Extraction took {time() - start} secs\")\n",
    "    return full_comment_metadata_videos\n",
    "\n",
    "def Transform(comments:\"list(dict)\",\n",
    "              video_names:list, videos_id:list) -> \"(pd.DataFrame, pd.DataFrame)\":\n",
    "    \n",
    "    '''\n",
    "    This functions transform the comments retrieved from youtube and metadata about\n",
    "    youtube video title and youtube video id and returns two parsed dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    comments: list(dict)\n",
    "        List of dictionaries containing the comments for each video and metadata\n",
    "        \n",
    "    video_names: list\n",
    "        List containing the titles of the youtube videos\n",
    "        \n",
    "    videos_id: list\n",
    "        List containing the ids of the youtube videos\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "        len(comments) = len(video_names) = len(videos_id)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        (pd.DataFrame, pd.DataFrame): A tuple containing the comments dataframe\n",
    "            and the video dataframe respectively\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"Transfoming data\")\n",
    "    start = time()\n",
    "\n",
    "    video_id_df = pd.DataFrame({\"id_video\": video_names,\"title\": videos_id})\n",
    "    comments_df = pd.DataFrame()\n",
    "\n",
    "    # Adding id_video where the comments came from\n",
    "    for comments_by_video, ID in zip(comments, videos_id):\n",
    "        df = pd.DataFrame(comments_by_video)\n",
    "        df[\"id_video\"] = ID\n",
    "        comments_df = pd.concat([comments_df, df])\n",
    "\n",
    "    comments_df = comments_df.reset_index(drop=True).copy()\n",
    "\n",
    "    # Casting correctly date time\n",
    "    date_series = comments_df[\"published_date\"].str.replace(\"(T|Z)\", \" \", regex=True)\n",
    "    date_format = pd.to_datetime(date_series, format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    comments_df[\"published_date\"] = date_format\n",
    "\n",
    "    print(f\"Transformation took {time() - start} secs\")\n",
    "\n",
    "    return comments_df, video_id_df\n",
    "\n",
    "def Clean(comments_df:pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    '''\n",
    "    This function remove emojis, html tags, html \"codes\" and non-printable characters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    comments_df: pd.DataFrame\n",
    "        A pandas DataFrame with the comments retrieved from youtube\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    pd.DataFrame: A pandas DataFrame cleaned\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"Cleaning data\")\n",
    "    start = time()\n",
    "    \n",
    "    # Transforming HTML \"codification\" to utf-8\n",
    "    map_columns_to_function = {\"comment\":html.unescape, \"name\":html.unescape}\n",
    "    comments_df[[\"comment\",\"name\"]] = comments_df[[\"comment\",\"name\"]].agg(map_columns_to_function)\n",
    "    \n",
    "    # Emoji code patterns\n",
    "    emoji_patterns = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U0001F300-\\U0001F5FF\"\n",
    "            u\"\\U0001F680-\\U0001F6FF\"\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "            u\"\\U00002500-\\U00002BEF\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\" \n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"\n",
    "            u\"\\u3030\"\n",
    "                          \"]+\", re.UNICODE)\n",
    "    \n",
    "    # Non printable characters\n",
    "    non_printable_patterns = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F\\r]')\n",
    "    \n",
    "    # Clean emojis, html tags and detect language\n",
    "    clean_text = []\n",
    "    for text in comments_df[\"comment\"]:\n",
    "        clean_emojis_text = emoji_patterns.sub(r\"\", text)\n",
    "        \n",
    "        # More expensive than ReGex but more useful using BeatifulSoup for html tags\n",
    "        clean_html_text = BeautifulSoup(clean_emojis_text, \"lxml\").text\n",
    "        clean_non_print = non_printable_patterns.sub(r\" \", clean_html_text)\n",
    "        clean_text.append(clean_non_print)\n",
    "        \n",
    "    comments_df[\"comment\"] = clean_text\n",
    "\n",
    "    print(f\"Cleaning took {time() - start} secs\")\n",
    "\n",
    "    return comments_df\n",
    "\n",
    "def Complement(comments_df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # This function try to recognize the language of the comments and added it into\n",
    "    # the dataframe\n",
    "    \n",
    "    # Detecting language\n",
    "    print(\"Detecting language\")\n",
    "    start = time()\n",
    "\n",
    "    comment_languages = []\n",
    "    for i, text in enumerate(comments_df[\"comment\"], start=1):\n",
    "\n",
    "        print(f\"\\tProcessing comment number {i}\", end=\"\\r\")\n",
    "\n",
    "        # langdetect doesn't work very well in short sentences. Threshold > 4\n",
    "        if len(re.split(\"\\s+\", text)) > 4:\n",
    "            language = GetLanguage(text)\n",
    "        else:\n",
    "            language = \"undefined\"\n",
    "\n",
    "        comment_languages.append(language)\n",
    "\n",
    "    comments_df[\"code\"] = comment_languages\n",
    "    \n",
    "    print(f\"\\nDetecting language took {(time() - start) / 60} min\")\n",
    "\n",
    "    return comments_df\n",
    "\n",
    "def Validate(tables:\"list(tuple)\", engine:sqlalchemy.create_engine, inplace:bool=False) -> \"list|(list, list(dict))\":\n",
    "\n",
    "    '''\n",
    "    This function validate if the data found in a dataframe is not violating any\n",
    "    constraint defined in the tables of the database\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    tables: list(tuple)\n",
    "        A list of tuple of 3 elements containing respectively\n",
    "        1. Name of the table in the database\n",
    "        2. ORM simulating table in the database created with sqlalchemy.orm.declarative_base\n",
    "        3. DataFrame where the validation will take place\n",
    "        \n",
    "    engine: sqlalchemy.create_engine\n",
    "        An sqlalchemy.create_engine object to connect to the database\n",
    "        \n",
    "    inplace:bool; default=False\n",
    "        Remove the registers not valids inplace\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \"list|(list, list(dict))\": Return a list containing the name of the indexes of the\n",
    "        DataFrame that should be dropped. if the argument 'inplace' is set True. It returns\n",
    "        a tuple containing the list plus a list of dictionaries containing the dataframes\n",
    "        modified where key is the database base name and value the modified dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    validated_registers_in_table = {}\n",
    "    for table_name, orm_model, df in tables:\n",
    "\n",
    "        print(f\"Validating registers in table: {table_name}\")\n",
    "        validator = DataValidator(engine, orm_model)\n",
    "        bad_rows = validator.ValidateData(df, errors=\"ignore\")\n",
    "\n",
    "        if inplace:\n",
    "            df.drop(bad_rows, axis=0,inplace=True)\n",
    "            validated_registers_in_table[table_name] = df\n",
    "    \n",
    "    return (bad_rows, validated_registers_in_table) if inplace else bad_rows\n",
    "            \n",
    "def Load(tables:dict, engine:sqlalchemy.create_engine) -> list:\n",
    "    \n",
    "    # This function takes a dict containing as keys the name of the tables in the database\n",
    "    # and values as a DataFrame to load into the database. Return the number of\n",
    "    # registers modified in the database\n",
    "    \n",
    "    modified_registers = []\n",
    "    for table_name in tables.keys():\n",
    "        \n",
    "        print(f\"Loading {table_name}\")\n",
    "        registers = tables[table_name].to_sql(table_name,\n",
    "                                              con=engine,\n",
    "                                              index=False,\n",
    "                                              if_exists=\"append\")\n",
    "        modified_registers.append(registers)\n",
    "\n",
    "    return modified_registers\n",
    "\n",
    "def ConnectDataBase() -> sqlalchemy.create_engine:\n",
    "    \n",
    "    username = input(\"User postgres:\")\n",
    "    host = input(\"Host:\")\n",
    "    database = input(\"Database:\")\n",
    "    password = getpass.getpass(\"Password:\")\n",
    "\n",
    "    engine = create_engine(f'postgresql://{username}:{password}@{host}/{database}')\n",
    "    return engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5526807",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Video title and ID for which I want to retrieve comments\n",
    "    video_names = [\"What Happened Before History? Human Origins\",\n",
    "                \"The Past We Can Never Return To - The Anthropocene Reviewed\",\n",
    "                \"Why Blue Whales Don't Get Cancer - Peto's Paradox\",\n",
    "                \"What If We Detonated All Nuclear Bombs at Once?\",\n",
    "                \"We WILL Fix Climate Change!\",\n",
    "                \"Building a Marsbase is a Horrible Idea: Let's do it!\",\n",
    "                \"What if We Nuke a City?\"]\n",
    "\n",
    "    videos_id = [\"dGiQaabX3_o\",\"YbgnlkJPga4\",\n",
    "                \"1AElONvi9WQ\",\"JyECrGp-Sw8\",\n",
    "                \"LxgMdjyw8uw\",\"uqKGREZs6-w\",\n",
    "                \"5iPH-br_eJQ\"]\n",
    "    \n",
    "    api_key = LoadKeyYotube()\n",
    "    \n",
    "    # Extract, convert into a dataframe, clean and add language of the comment\n",
    "    comments_dict = Extract(video_names, videos_id, api_key)\n",
    "    comments_df, titles_df = Transform(comments_dict, video_names, videos_id)\n",
    "    comments_df = Clean(comments_df)\n",
    "    comments_df = Complement(comments_df)\n",
    "\n",
    "    # Tables without foreign keys in the data base\n",
    "    languages_df = pd.DataFrame(comments_df[\"code\"].unique(), columns=[\"code\"])\n",
    "    titles_df = pd.DataFrame({\"id_video\":videos_id, \"title\":video_names})\n",
    "    users_df = pd.DataFrame(comments_df[\"name\"].unique(), columns=[\"name\"])\n",
    "\n",
    "    tables_isolated = [(\"languages\", Languages, languages_df),\n",
    "                    (\"titles\", Titles, titles_df),\n",
    "                    (\"users\", Users, users_df)]\n",
    "    \n",
    "    engine = ConnectDataBase()\n",
    "\n",
    "    # Validate and load dataframes into the postgres database\n",
    "    _, tables_isolated_validated = Validate(tables_isolated, engine, True)\n",
    "    _ = Load(tables_isolated_validated, engine)\n",
    "\n",
    "    # Retrieve id from new registers added by the \"isolated\" tables\n",
    "    registers_languages = pd.read_sql(\"SELECT * FROM languages\", engine)\n",
    "    registers_users = pd.read_sql(\"SELECT * FROM users\", engine)\n",
    "\n",
    "    # Mapping with the appropiate values in the foreign keys columns\n",
    "    # and the new registers added\n",
    "    comments_df_mapped = comments_df.copy().merge(registers_languages,\n",
    "                                                    how=\"left\",\n",
    "                                                    on=\"code\")\n",
    "\n",
    "    comments_df_mapped.drop([\"code\"], axis=1, inplace=True)\n",
    "\n",
    "    comments_df_mapped = comments_df_mapped.merge(registers_users,\n",
    "                                                how=\"left\",\n",
    "                                                on=\"name\")\n",
    "\n",
    "    comments_df_mapped.drop([\"name\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Table with foreign key\n",
    "    tables_no_isolated = [(\"comments\", Comments, comments_df_mapped)]\n",
    "\n",
    "    _, tables_no_isolated_validated = Validate(tables_no_isolated, engine, True)\n",
    "    _ = Load(tables_no_isolated_validated, engine)\n",
    "\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a03460-8681-4f4e-9040-3009fef1c84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
